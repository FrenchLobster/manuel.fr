---
sidebar_position: 7
title: "Import SafeSensor"
description: "Description pas à pas de l'import d'un modèle SafeSensor dans Ollama depuis Hunging Face"
---

# Importer un modèle SafeSensor depuis Hunging Face

La méthode d'importation de safetensor dans Ollama fonctionne pour un ensemble limité de modèles. Vous pouvez importer des modèles qu'Ollama ne prend pas en charge en utilisant llama.cpp pour convertir les safetensors au format GGUF.

Avec Docker, on convertit par exemple un modèle ainsi :

```bash
docker run --rm -it -v .:/app/models ghcr.io/ggerganov/llama.cpp:full-cuda -c --outtype f16 /app/models
```
Cela crée un fichier (Models-12B-F16.gguf) dans le dossier courant, que l'on passe à Ollama

```bash
echo FROM Models-12B-F16.gguf > Modelfile
ollama create magnum-v2-12b
```
Ollama detecte le template Chat et renseigne les paramettres

```bash
$ ollama show --modelfile magnum-v2-12b
# Modelfile generated by "ollama show"
# To build a new Modelfile based on this, replace FROM with:
# FROM magnum-v2-12b:latest

FROM /root/.ollama/models/blobs/sha256-587d03f008224912b27034e98665dfbb8347f9b9eaa01d2e9968bb0299d5a72e
TEMPLATE "{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
{{ .Response }}<|im_end|>
"
PARAMETER stop <|im_start|>
PARAMETER stop <|im_end|>
```

Faites attention que les LLM en fp16 prennent plus de ressources en mémoire pour fonctionner
```bash
$ ollama run magnum-v2-12b
/>/>/> hello
Hello! How can I help you today?

/>/>/> /bye
$ ollama ps
NAME                    ID              SIZE    PROCESSOR       UNTIL   
magnum-v2-12b:latest    0daea775ee7d    25 GB   36%/64% CPU/GPU Forever
```